{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "# Feedback Loop in RAG\n",
    "\n",
    "이 노트북에서는 시간이 지남에 따라 지속적으로 개선되는 피드백 루프 메커니즘을 갖춘 RAG 시스템을 구현하겠습니다. 사용자 피드백을 수집하고 반영함으로써, 우리 시스템은 매 상호작용마다 더 관련성 있고 품질 높은 응답을 제공하는 법을 배웁니다.\n",
    "\n",
    "전통적인 RAG 시스템은 정적이어서, 임베딩 유사도에만 기반해 정보를 찾아줍니다. 하지만 피드백 루프를 도입하면, 시스템은 이렇게 달라집니다:\n",
    "\n",
    "1. 잘 작동했던 방식과 그렇지 않았던 방식을 기억하고\n",
    "2. 시간이 지날수록 문서의 중요도를 조정하며\n",
    "3. 효과적인 질문과 답변 쌍을 지식으로 쌓고\n",
    "4.사용자와의 상호작용을 통해 점점 더 똑똑해집니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 환경 설정하기\n",
    "필요한 라이브러리를 가져옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "from datetime import datetime\n",
    "\n",
    "# .env 파일 로드\n",
    "load_dotenv()\n",
    "\n",
    "API_KEY = os.environ.get('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF 파일에서 텍스트 추출하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    # API 키 설정\n",
    "    genai.configure(api_key=gemini_API_KEY)\n",
    "    client = genai.GenerativeModel('gemini-2.0-flash-lite')\n",
    "\n",
    "    # PDF 파일 업로드\n",
    "    with open(pdf_path, \"rb\") as file:\n",
    "        file_data = file.read()\n",
    "\n",
    "\n",
    "    prompt = \"Extract all text from the provided PDF file.\"\n",
    "    response = client.generate_content([\n",
    "        {\"mime_type\": \"application/pdf\", \"data\": file_data},\n",
    "        prompt\n",
    "    ],generation_config={\n",
    "            \"max_output_tokens\": 8192  # 최대 출력 토큰 수 설정 (예: 8192 토큰, 약 24,000~32,000자)\n",
    "    })\n",
    "    return response.text\n",
    "    # Open the PDF file\n",
    "    mypdf = fitz.open(pdf_path)\n",
    "    all_text = \"\"  # Initialize an empty string to store the extracted text\n",
    "\n",
    "    # Iterate through each page in the PDF\n",
    "    for page_num in range(mypdf.page_count):\n",
    "        page = mypdf[page_num]  # Get the page\n",
    "        text = page.get_text(\"text\")  # Extract text from the page\n",
    "        all_text += text  # Append the extracted text to the all_text string\n",
    "\n",
    "    return all_text  # Return the extracted text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이미 text 파일로 저장되어 있다면 load_text_file 함수를 사용하면 됩니다.\n",
    "def load_text_file(pdf_path):\n",
    "\n",
    "    # text 파일 로드\n",
    "    with open(pdf_path, \"r\", encoding=\"utf-8\") as txt_file:\n",
    "        text = txt_file.read()\n",
    "\n",
    "    return text\n",
    "\n",
    "txt_path = \"./data_creation/pdf_data/(1) 2024 달라지는 세금제도.txt\"\n",
    "\n",
    "extracted_text = load_text_file(txt_path)\n",
    "print(extracted_text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 추출된 텍스트 chunking\n",
    "pdf 에서 추출된 텍스트를 얻은 후, 검색 정확성을 향상시키기 위해 이를 더 작고 겹치는 청크로 나눕니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, n, overlap):\n",
    "    \"\"\"\n",
    "    주어진 텍스트를 겹치는 n개의 문자 세그먼트로 청크합니다.\n",
    "\n",
    "    Args:\n",
    "    text (str): 청크할 텍스트입니다.\n",
    "    n (int): 각 청크의 문자 수입니다.\n",
    "    overlap (int): 청크 간 겹치는 문자 수입니다.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: 청크된 텍스트 리스트입니다.\n",
    "    \"\"\"\n",
    "    chunks = []  # 청크된 텍스트를 저장할 빈 리스트를 초기화합니다.\n",
    "    \n",
    "    # (n - overlap) 단계로 텍스트를 반복합니다.\n",
    "    for i in range(0, len(text), n - overlap):\n",
    "        # 인덱스 i부터 i + n까지의 텍스트를 청크 리스트에 추가합니다.\n",
    "        chunks.append(text[i:i + n])\n",
    "\n",
    "    return chunks  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI API 클라이언트 설정하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client_openai = OpenAI(api_key = API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Vector Store 구축\n",
    "NumPy를 사용하여 간단한 Vecotr store 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleVectorStore:\n",
    "    \"\"\"\n",
    "    NumPy를 사용하여 간단한 Vecotr store 구축\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        벡터 저장소 초기화\n",
    "        \"\"\"\n",
    "        self.vectors = []\n",
    "        self.texts = []\n",
    "        self.metadata = []\n",
    "    \n",
    "    def add_item(self, text, embedding, metadata=None):\n",
    "        \"\"\"\n",
    "        벡터 저장소에 항목 추가\n",
    "\n",
    "        Args:\n",
    "        text (str): 원본 텍스트.\n",
    "        embedding (List[float]): 임베딩 벡터.\n",
    "        metadata (dict, optional): 추가 메타데이터.\n",
    "        \"\"\"\n",
    "        self.vectors.append(np.array(embedding))\n",
    "        self.texts.append(text)\n",
    "        self.metadata.append(metadata or {})\n",
    "    \n",
    "    def similarity_search(self, query_embedding, k=5, filter_func=None):\n",
    "        \"\"\"\n",
    "        쿼리 임베딩과 가장 비슷한 항목들을 코사인 유사도를 이용해 찾는 함수\n",
    "\n",
    "        Args:\n",
    "            query_embedding (List[float]): 쿼리 임베딩 벡터.\n",
    "            k (int): 반환할 결과의 수.\n",
    "            filter_func (callable, optional): 메타데이터를 기반으로 결과를 필터링하는 함수.\n",
    "                                            메타데이터 딕셔너리를 입력으로 받아 boolean을 반환.\n",
    "\n",
    "        Returns:\n",
    "            List[Dict]: 상위 k개 유사 항목, 각 항목은 다음을 포함:\n",
    "                - text: 원본 텍스트\n",
    "                - metadata: 연관된 메타데이터\n",
    "                - similarity: 코사인 유사도 점수\n",
    "                - relevance_score: 메타데이터 기반 관련성 또는 계산된 유사도\n",
    "                \n",
    "        Note: 벡터가 저장되어 있지 않거나 필터를 통과하지 못하면 빈 리스트를 반환.\n",
    "        \"\"\"\n",
    "        if not self.vectors:\n",
    "            return []  # 벡터 저장소가 비어있으면 빈 리스트 반환\n",
    "        \n",
    "        # 쿼리 임베딩을 numpy 배열로 변환\n",
    "        query_vector = np.array(query_embedding)\n",
    "        \n",
    "        # 쿼리와 각 저장된 벡터 사이의 코사인 유사도 계산\n",
    "        similarities = []\n",
    "        for i, vector in enumerate(self.vectors):\n",
    "            # 필터 기준을 통과하지 못하는 항목 건너뛰기\n",
    "            if filter_func and not filter_func(self.metadata[i]):\n",
    "                continue\n",
    "                \n",
    "            # 코사인 유사도 계산: dot product / (norm1 * norm2)\n",
    "            similarity = np.dot(query_vector, vector) / (np.linalg.norm(query_vector) * np.linalg.norm(vector))\n",
    "            similarities.append((i, similarity))  # 인덱스와 유사도 점수 저장\n",
    "        \n",
    "        # 유사도 점수에 따라 정렬 (내림차순)\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # 상위 k개 결과 딕셔너리 생성\n",
    "        results = []\n",
    "        for i in range(min(k, len(similarities))):\n",
    "            idx, score = similarities[i]\n",
    "            results.append({\n",
    "                \"text\": self.texts[idx],\n",
    "                \"metadata\": self.metadata[idx],\n",
    "                \"similarity\": score,\n",
    "                # 메타데이터에서 이미 존재하는 관련성 점수가 있으면 사용, 없으면 유사도 점수 사용\n",
    "                \"relevance_score\": self.metadata[idx].get(\"relevance_score\", score)\n",
    "            })\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 임베딩 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "def create_embeddings(embedding_model, texts, device='cuda', batch_size=16):\n",
    "    \"\"\"\n",
    "    SentenceTransformer 모델을 사용하여 지정된 텍스트에 대한 임베딩을 생성합니다.\n",
    "\n",
    "    Args:\n",
    "        embedding_model: 임베딩을 생성할 SentenceTransformer 모델입니다.\n",
    "        texts (list): 임베딩을 생성할 입력 텍스트 리스트입니다.\n",
    "        device (str): 모델을 실행할 장치 ('cuda' for GPU, 'cpu' for CPU).\n",
    "        batch_size (int): 인코딩을 위한 배치 크기입니다.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: 모델에 의해 생성된 임베딩입니다.\n",
    "    \"\"\"\n",
    "    # 모델이 지정된 장치에 있는지 확인합니다.\n",
    "    embedding_model = embedding_model.to(device)\n",
    "    \n",
    "    # 지정된 배치 크기로 임베딩을 생성합니다.\n",
    "    embeddings = embedding_model.encode(\n",
    "        texts,\n",
    "        device=device,\n",
    "        batch_size=batch_size,  # 메모리 사용량을 줄이기 위해 더 작은 배치 크기를 사용합니다.\n",
    "        show_progress_bar=True  # 인코딩 진행 상태를 모니터링하기 위한 진행 표시줄을 표시합니다.\n",
    "    )\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "# GPU 사용 가능 여부를 확인합니다.\n",
    "device = 'cuda' if torch.cuda.is_available() else 'mps'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 모델을 로드합니다.\n",
    "model = \"BAAI/bge-m3\"\n",
    "embedding_model = SentenceTransformer(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feedback System Functions\n",
    "이제 핵심 피드백 시스템 구성 요소들을 구현해보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_user_feedback(query, response, relevance, quality, comments=\"\"):\n",
    "    \"\"\"\n",
    "    사용자 피드백을 딕셔너리 형태로 포맷팅.\n",
    "    \n",
    "    Args:\n",
    "        query (str): 사용자 쿼리\n",
    "        response (str): 시스템 응답\n",
    "        relevance (int): 관련성 점수 (1-5)\n",
    "        quality (int): 품질 점수 (1-5)\n",
    "        comments (str): 선택적 피드백 코멘트\n",
    "        \n",
    "    Returns:\n",
    "        Dict: 포맷팅된 피드백\n",
    "    \"\"\"\n",
    "    return {\n",
    "        \"query\": query,\n",
    "        \"response\": response,\n",
    "        \"relevance\": int(relevance),\n",
    "        \"quality\": int(quality),\n",
    "        \"comments\": comments,\n",
    "        \"timestamp\": datetime.now().isoformat()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def store_feedback(feedback, feedback_file=\"feedback_data.json\"):\n",
    "    \"\"\"\n",
    "    피드백을 JSON 파일에 저장.\n",
    "    \n",
    "    Args:\n",
    "        feedback (Dict): 피드백 데이터\n",
    "        feedback_file (str): 피드백 파일 경로\n",
    "    \"\"\"\n",
    "    with open(feedback_file, \"a\") as f:\n",
    "        json.dump(feedback, f)\n",
    "        f.write(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "def load_feedback_data(feedback_file=\"feedback_data.json\"):\n",
    "    \"\"\"\n",
    "    피드백 데이터를 파일에서 로드.\n",
    "    \n",
    "    Args:\n",
    "        feedback_file (str): feedback file 경로\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: 피드백 항목 리스트\n",
    "    \"\"\"\n",
    "    feedback_data = []\n",
    "    try:\n",
    "        with open(feedback_file, \"r\") as f:\n",
    "            for line in f:\n",
    "                if line.strip():\n",
    "                    feedback_data.append(json.loads(line.strip()))\n",
    "    except FileNotFoundError:\n",
    "        print(\"No feedback data file found. Starting with empty feedback.\")\n",
    "    \n",
    "    return feedback_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Processing with Feedback Awareness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(file_path, chunk_size=1000, chunk_overlap=200):\n",
    "    \"\"\"\n",
    "    피드백 루프를 포함한 RAG(검색 기반 생성)용 문서 처리\n",
    "\n",
    "    이 함수는 전체 문서 처리 과정을 담당합니다.\n",
    "    1. PDF에서 텍스트 추출\n",
    "    2. 텍스트를 겹치게 청크로 분할\n",
    "    3. 각 청크에 대한 임베딩 생성\n",
    "    4. 메타데이터와 함께 벡터 데이터베이스에 저장\n",
    "\n",
    "    Args:\n",
    "    file_path (str): 파일 경로\n",
    "    chunk_size (int): 각 텍스트 청크의 크기(문자)\n",
    "    chunk_overlap (int): 연속된 청크 간 중복 범위(문자)\n",
    "\n",
    "    Returns:\n",
    "    Tuple[List[str], SimpleVectorStore]: A tuple containing:\n",
    "        - 문서 청크 리스트\n",
    "        - 임베딩과 메타데이터가 포함된 벡터 저장소\n",
    "    \"\"\"\n",
    "    # 1. PDF에서 텍스트 추출\n",
    "    #print(\"Extracting text from PDF...\")\n",
    "    #extracted_text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "    # 1. 텍스트 파일 로드\n",
    "    extracted_text = load_text_file(file_path)\n",
    "    \n",
    "    # 2. 청크로 분할\n",
    "    print(\"Chunking text...\")\n",
    "    chunks = chunk_text(extracted_text, chunk_size, chunk_overlap)\n",
    "    print(f\"Created {len(chunks)} text chunks\")\n",
    "    \n",
    "    # 3. 각 청크에 대한 임베딩 생성\n",
    "    print(\"Creating embeddings for chunks...\")\n",
    "    chunk_embeddings = create_embeddings(embedding_model, chunks, device=device, batch_size=1)\n",
    "    \n",
    "    # 4. 벡터 저장소 초기화\n",
    "    store = SimpleVectorStore()\n",
    "    \n",
    "    # 5. 각 청크와 임베딩을 벡터 저장소에 추가\n",
    "    # 피드백 기반 개선을 위한 메타데이터 포함\n",
    "    for i, (chunk, embedding) in enumerate(zip(chunks, chunk_embeddings)):\n",
    "        store.add_item(\n",
    "            text=chunk,\n",
    "            embedding=embedding,\n",
    "            metadata={\n",
    "                \"index\": i,                # 원본 문서에서의 위치\n",
    "                \"source\": file_path,        # 원본 문서 경로\n",
    "                \"relevance_score\": 1.0,    # 초기 관련성 점수 (피드백에 따라 업데이트됨)\n",
    "                \"feedback_count\": 0        # 이 청크에 대해 받은 피드백 수\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    print(f\"Added {len(chunks)} chunks to the vector store\")\n",
    "    return chunks, store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relevance Adjustment Based on Feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_feedback_relevance(query, doc_text, feedback):\n",
    "    \"\"\"\n",
    "    LLM을 활용해 이전 피드백이 현재 쿼리와 문서에 관련 있는지 평가합니다.\n",
    "    \n",
    "    이 함수는 현재 검색에 어떤 과거 피드백이 영향을 미쳐야 하는지 판단하는 데 도움을 줍니다.\n",
    "    \n",
    "    현재 쿼리, 과거 쿼리와 피드백, 그리고 문서 내용을 LLM에 전달해 관련성을 평가받습니다.\n",
    "    \n",
    "    Args:\n",
    "        query (str): 현재 사용자 쿼리\n",
    "        doc_text (str): 평가할 문서의 텍스트 내용\n",
    "        feedback (Dict): 'query'와 'response' 키가 포함된 이전 피드백 데이터\n",
    "        \n",
    "    Returns:\n",
    "        bool: 피드백이 현재 쿼리와 문서와 관련 있는지 여부\n",
    "    \"\"\"\n",
    "    # 시스템 프롬프트 정의\n",
    "    system_prompt = \"\"\"You are an AI system that determines if a past feedback is relevant to a current query and document.\n",
    "    Answer with ONLY 'yes' or 'no'. Your job is strictly to determine relevance, not to provide explanations.\"\"\"\n",
    "\n",
    "    # 사용자 프롬프트 생성\n",
    "    user_prompt = f\"\"\"\n",
    "    Current query: {query}\n",
    "    Past query that received feedback: {feedback['query']}\n",
    "    Document content: {doc_text[:500]}... [truncated]\n",
    "    Past response that received feedback: {feedback['response'][:500]}... [truncated]\n",
    "\n",
    "    Is this past feedback relevant to the current query and document? (yes/no)\n",
    "    \"\"\"\n",
    "\n",
    "    # LLM API 호출\n",
    "    response = client_openai.chat.completions.create(\n",
    "        model=\"gpt-4.1-mini\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ],\n",
    "        temperature=0  \n",
    "    )\n",
    "    \n",
    "    # 응답 추출 및 정규화\n",
    "    answer = response.choices[0].message.content.strip().lower()\n",
    "    return 'yes' in answer  # 'yes'가 포함되어 있으면 True 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adjust_relevance_scores(query, results, feedback_data):\n",
    "    \"\"\"\n",
    "    과거 피드백을 반영해 문서의 관련성 점수를 조정하여 검색 품질을 향상시킵니다.\n",
    "    \n",
    "    이 함수는 이전 사용자 피드백을 분석해, 현재 쿼리와 관련된 피드백을 찾아내고\n",
    "    피드백의 관련성 평가에 따라 점수 보정값을 계산한 뒤, 결과를 다시 정렬합니다.\n",
    "    \n",
    "    Args:\n",
    "        query (str): 현재 사용자 쿼리\n",
    "        results (List[Dict]): 원본 유사도 점수가 포함된 검색된 문서\n",
    "        feedback_data (List[Dict]): 사용자 평가가 포함된 과거 피드백\n",
    "        \n",
    "    Returns:\n",
    "        List[Dict]: 관련성 점수가 조정된 결과, 새로운 점수에 따라 정렬됨\n",
    "    \"\"\"\n",
    "    # 피드백 데이터가 없으면 원본 결과 반환\n",
    "    if not feedback_data:\n",
    "        return results\n",
    "    \n",
    "    print(\"Adjusting relevance scores based on feedback history...\")\n",
    "    \n",
    "    # Process each retrieved document\n",
    "    for i, result in enumerate(results):\n",
    "        document_text = result[\"text\"]\n",
    "        relevant_feedback = []\n",
    "        \n",
    "        # 이 특정 문서와 쿼리 조합에 대한 관련성 있는 피드백 찾기\n",
    "        # LLM을 사용하여 각 과거 피드백 항목의 관련성 평가\n",
    "        for feedback in feedback_data:\n",
    "            is_relevant = assess_feedback_relevance(query, document_text, feedback)\n",
    "            if is_relevant:\n",
    "                relevant_feedback.append(feedback)\n",
    "        \n",
    "        # 관련성 있는 피드백이 있으면 점수 조정\n",
    "        if relevant_feedback:\n",
    "            # 모든 적용 가능한 피드백 항목에서 평균 관련성 등급 계산\n",
    "            # 피드백 관련성은 1-5 범위 (1=관련 없음, 5=매우 관련 있음)\n",
    "            avg_relevance = sum(f['relevance'] for f in relevant_feedback) / len(relevant_feedback)\n",
    "            \n",
    "            # 평균 관련성을 0.5-1.5 범위의 점수 수정자로 변환\n",
    "            # - 3/5 미만의 점수는 원래 유사도를 감소시킴 (수정자 < 1.0)\n",
    "            # - 3/5 이상의 점수는 원래 유사도를 증가시킴 (수정자 > 1.0)\n",
    "            modifier = 0.5 + (avg_relevance / 5.0)\n",
    "            \n",
    "            # 원래 유사도에 수정자 적용\n",
    "            original_score = result[\"similarity\"]\n",
    "            adjusted_score = original_score * modifier\n",
    "            \n",
    "            # 결과 딕셔너리에 새로운 점수와 피드백 메타데이터 업데이트\n",
    "            result[\"original_similarity\"] = original_score  # 원래 유사도 유지\n",
    "            result[\"similarity\"] = adjusted_score           # 기본 점수 업데이트\n",
    "            result[\"relevance_score\"] = adjusted_score      # 관련성 점수 업데이트\n",
    "            result[\"feedback_applied\"] = True               # 피드백 적용 플래그\n",
    "            result[\"feedback_count\"] = len(relevant_feedback)  # 사용된 피드백 항목 수\n",
    "            \n",
    "            # 조정에 대한 세부 정보 logging\n",
    "            print(f\"  Document {i+1}: Adjusted score from {original_score:.4f} to {adjusted_score:.4f} based on {len(relevant_feedback)} feedback(s)\")\n",
    "    \n",
    "    # 조정된 점수에 따라 결과 다시 정렬\n",
    "    results.sort(key=lambda x: x[\"similarity\"], reverse=True)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fine-tuning Our Index with Feedback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_index(current_store, chunks, feedback_data):\n",
    "    \"\"\"\n",
    "    고품질 피드백을 사용하여 검색 품질을 향상시키기 위해 벡터 저장소를 개선합니다.\n",
    "    \n",
    "    이 함수는 다음과 같은 연속적인 학습 과정을 구현합니다:\n",
    "    1. 고품질 피드백 (높은 평가 점수의 Q&A 쌍) 식별\n",
    "    2. 성공적인 상호작용에서 새로운 검색 항목 생성\n",
    "    3. 관련성 가중치가 증가된 벡터 저장소에 추가\n",
    "    \n",
    "    Args:\n",
    "        current_store (SimpleVectorStore): 원본 문서 청크가 포함된 현재 벡터 저장소\n",
    "        chunks (List[str]): 원본 문서 텍스트 청크\n",
    "        feedback_data (List[Dict]): 관련성과 품질 등급이 포함된 과거 사용자 피드백\n",
    "        \n",
    "    Returns:\n",
    "        SimpleVectorStore: 원본 청크와 피드백 기반 콘텐츠가 포함된 개선된 벡터 저장소\n",
    "    \"\"\"\n",
    "    print(\"Fine-tuning index with high-quality feedback...\")\n",
    "    \n",
    "    # 고품질 응답만 필터링 (관련성과 품질 등급이 4 또는 5)\n",
    "    # 이는 가장 성공적인 상호작용만 학습하도록 보장합니다\n",
    "    good_feedback = [f for f in feedback_data if f['relevance'] >= 4 and f['quality'] >= 4]\n",
    "    \n",
    "    if not good_feedback:\n",
    "        print(\"No high-quality feedback found for fine-tuning.\")\n",
    "        return current_store  # 좋은 피드백이 없으면 원본 저장소 반환\n",
    "    \n",
    "    # 원본 청크와 피드백 기반 콘텐츠가 포함된 새로운 저장소 초기화\n",
    "    new_store = SimpleVectorStore()\n",
    "    \n",
    "    # 먼저 원본 문서 청크와 기존 메타데이터를 전송\n",
    "    for i in range(len(current_store.texts)):\n",
    "        new_store.add_item(\n",
    "            text=current_store.texts[i],\n",
    "            embedding=current_store.vectors[i],\n",
    "            metadata=current_store.metadata[i].copy()  # 참조 문제를 방지하기 위해 복사 사용\n",
    "        )\n",
    "    \n",
    "    # 좋은 피드백에서 향상된 콘텐츠 생성 및 추가\n",
    "    for feedback in good_feedback:\n",
    "        # 질문과 고품질 답변을 결합하는 새로운 문서 포맷팅\n",
    "        # 이는 사용자 쿼리에 직접 주소를 지정하는 검색 가능한 콘텐츠를 생성합니다\n",
    "        enhanced_text = f\"Question: {feedback['query']}\\nAnswer: {feedback['response']}\"\n",
    "        \n",
    "        # 이 새로운 합성 문서에 대한 임베딩 벡터 생성\n",
    "        embedding = create_embeddings(enhanced_text)\n",
    "        \n",
    "        # 원본 출처와 중요성을 식별하는 특수 메타데이터로 벡터 저장소에 추가\n",
    "        new_store.add_item(\n",
    "            text=enhanced_text,\n",
    "            embedding=embedding,\n",
    "            metadata={\n",
    "                \"type\": \"feedback_enhanced\",  # 피드백에서 파생됨\n",
    "                \"query\": feedback[\"query\"],   # 원본 쿼리 저장\n",
    "                \"relevance_score\": 1.2,       # 초기 관련성 증가\n",
    "                \"feedback_count\": 1,          # 피드백 통합 추적\n",
    "                \"original_feedback\": feedback # 완전한 피드백 레코드 유지\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        print(f\"Added enhanced content from feedback: {feedback['query'][:50]}...\")\n",
    "    \n",
    "    # 개선에 대한 요약 통계 logging\n",
    "    print(f\"Fine-tuned index now has {len(new_store.texts)} items (original: {len(chunks)})\")\n",
    "    return new_store"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete RAG Pipeline with Feedback Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query, context, model_name='gpt-4.1-mini'):\n",
    "\n",
    "    # AI 어시스턴트의 시스템 프롬프트를 정의합니다.\n",
    "    system_prompt = \"당신은 제공된 Context에 기반하여 답변하는 AI 어시스턴트입니다. 답변이 컨텍스트에서 직접 도출될 수 없는 경우, 다음 문장을 사용하세요: '해당 질문에 답변할 충분한 정보가 없습니다.'\"\n",
    "    \n",
    "    user_prompt = f\"\"\"\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        Question: {query}\n",
    "\n",
    "        Please answer the question based only on the context provided above. Be concise and accurate.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = client_openai.chat.completions.create(\n",
    "        model=model_name,\n",
    "        temperature=0.1,\n",
    "        top_p=0.9,\n",
    "        max_tokens=1024,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_with_feedback_loop(query, vector_store, feedback_data, k=5, model_name=\"gpt-4.1-mini\"):\n",
    "    \"\"\"\n",
    "    피드백 루프를 포함한 완전한 RAG 파이프라인을 구현합니다.    \n",
    "    \n",
    "    Args:\n",
    "        query (str): 사용자 쿼리\n",
    "        vector_store (SimpleVectorStore): 문서 청크가 포함된 벡터 저장소\n",
    "        feedback_data (List[Dict]): 피드백 기록\n",
    "        k (int): 검색할 문서 수\n",
    "        model (str): 응답 생성을 위한 LLM 모델\n",
    "        \n",
    "    Returns:\n",
    "        Dict: 쿼리, 검색된 문서, 응답이 포함된 결과\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== Processing query with feedback-enhanced RAG ===\")\n",
    "    print(f\"Query: {query}\")\n",
    "    \n",
    "    # 1. 쿼리 임베딩 생성\n",
    "    query_embedding = create_embeddings(embedding_model, [query], device=device, batch_size=1)[0]\n",
    "    \n",
    "    # 2. 쿼리 임베딩을 기반으로 초기 검색 수행\n",
    "    results = vector_store.similarity_search(query_embedding, k=k)\n",
    "    \n",
    "    # 3. 피드백을 기반으로 검색된 문서의 관련성 점수 조정\n",
    "    adjusted_results = adjust_relevance_scores(query, results, feedback_data)\n",
    "    \n",
    "    # 4. 조정된 결과에서 텍스트 추출하여 컨텍스트 생성\n",
    "    retrieved_texts = [result[\"text\"] for result in adjusted_results]\n",
    "    \n",
    "    # 5. 검색된 텍스트를 연결하여 응답 생성을 위한 컨텍스트 생성\n",
    "    context = \"\\n\\n---\\n\\n\".join(retrieved_texts)\n",
    "    \n",
    "    # 6. 컨텍스트와 쿼리를 사용하여 응답 생성\n",
    "    print(\"Generating response...\")\n",
    "    response = generate_response(query, context, model_name)\n",
    "    \n",
    "    # 7. 최종 결과 컴파일\n",
    "    result = {\n",
    "        \"query\": query,\n",
    "        \"retrieved_documents\": adjusted_results,\n",
    "        \"response\": response\n",
    "    }\n",
    "    \n",
    "    print(\"\\n=== Response ===\")\n",
    "    print(response)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete Workflow: From Initial Setup to Feedback Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def full_rag_workflow(file_path, query, feedback_data=None, feedback_file=\"feedback_data.json\", fine_tune=False):\n",
    "    \"\"\"\n",
    "    피드백 통합을 포함한 완전한 RAG 워크플로우를 실행합니다.\n",
    "    \n",
    "    이 함수는 전체 RAG(검색 기반 생성) 과정을 총괄합니다:\n",
    "    1. 과거 피드백 데이터를 불러옵니다\n",
    "    2. 문서를 처리하고 청크로 나눕니다\n",
    "    3. 필요하다면 이전 피드백을 활용해 벡터 인덱스를 미세 조정합니다\n",
    "    4. 피드백이 반영된 관련성 점수로 검색 및 생성을 수행합니다\n",
    "    5. 향후 개선을 위해 새로운 사용자 피드백을 수집합니다\n",
    "    6. 시스템이 점진적으로 학습할 수 있도록 피드백을 저장합니다\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): 처리할 파일 경로\n",
    "        query (str): 사용자 자연어 쿼리\n",
    "        feedback_data (List[Dict], optional): 이미 로드된 피드백 데이터, None인 경우 파일에서 로드\n",
    "        feedback_file (str): 피드백 기록을 저장하는 JSON 파일 경로\n",
    "        fine_tune (bool): 성공적인 과거 Q&A 쌍을 활용하여 인덱스를 개선할지 여부\n",
    "        \n",
    "    Returns:\n",
    "        Dict: 응답과 검색 메타데이터가 포함된 결과\n",
    "    \"\"\"\n",
    "    # 1. 관련성 조정을 위해 과거 피드백 데이터 로드\n",
    "    if feedback_data is None:\n",
    "        feedback_data = load_feedback_data(feedback_file)\n",
    "        print(f\"Loaded {len(feedback_data)} feedback entries from {feedback_file}\")\n",
    "    \n",
    "    # 2. 문서 처리, 청크 분할 및 임베딩 파이프라인 통과\n",
    "    chunks, vector_store = process_document(file_path)\n",
    "    \n",
    "    # 3. 고품질의 과거 상호작용 결과를 활용하여 벡터 인덱스 미세 조정\n",
    "    # 이는 성공적인 Q&A 쌍에서 향상된 검색 가능한 콘텐츠를 생성합니다\n",
    "    if fine_tune and feedback_data:\n",
    "        vector_store = fine_tune_index(vector_store, chunks, feedback_data)\n",
    "    \n",
    "    # 4. 피드백 인식 검색을 포함한 핵심 RAG 실행\n",
    "    # 참고: 이는 rag_with_feedback_loop 함수에 따라 달라집니다\n",
    "    result = rag_with_feedback_loop(query, vector_store, feedback_data)\n",
    "    \n",
    "    # 5. 향후 성능 향상을 위해 사용자 피드백 수집\n",
    "    print(\"\\n=== Would you like to provide feedback on this response? ===\")\n",
    "    print(\"Rate relevance (1-5, with 5 being most relevant):\")\n",
    "    relevance = input()\n",
    "    \n",
    "    print(\"Rate quality (1-5, with 5 being highest quality):\")\n",
    "    quality = input()\n",
    "    \n",
    "    print(\"Any comments? (optional, press Enter to skip)\")\n",
    "    comments = input()\n",
    "    \n",
    "    # 6. 피드백을 구조화된 데이터로 포맷팅\n",
    "    feedback = get_user_feedback(\n",
    "        query=query,\n",
    "        response=result[\"response\"],\n",
    "        relevance=int(relevance),\n",
    "        quality=int(quality),\n",
    "        comments=comments\n",
    "    )\n",
    "    \n",
    "    # 7. 피드백을 지속적으로 학습할 수 있도록 저장\n",
    "    store_feedback(feedback, feedback_file)\n",
    "    print(\"Feedback recorded. Thank you!\")\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating Our Feedback Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_feedback_loop(file_path, test_queries, reference_answers=None):\n",
    "    \"\"\"\n",
    "    피드백 루프가 RAG 품질에 미치는 영향을, 피드백 적용 전후의 성능을 비교해 평가합니다.\n",
    "    \n",
    "    이 함수는 피드백 통합이 검색 및 생성 결과에 어떤 영향을 주는지 실험적으로 측정합니다:\n",
    "    1. 1차: 모든 테스트 쿼리를 피드백 없이 실행\n",
    "    2. 기준 답변이 있다면 이를 바탕으로 인위적인 피드백 생성\n",
    "    3. 2차: 동일한 쿼리를 피드백이 반영된 상태로 실행\n",
    "    4. 두 결과를 비교해 피드백의 효과를 정량적으로 평가\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): 파일 경로\n",
    "        test_queries (List[str]): 시스템 성능을 평가하기 위한 테스트 쿼리 목록\n",
    "        reference_answers (List[str], optional): 평가 및 인위적인 피드백 생성을 위한 기준/골드 스탠다드 답변\n",
    "        \n",
    "    Returns:\n",
    "        Dict: 평가 결과가 포함된 결과:\n",
    "            - round1_results: 피드백 없는 결과\n",
    "            - round2_results: 피드백이 반영된 결과\n",
    "            - comparison: 라운드 간 정량적 비교 지표\n",
    "    \"\"\"\n",
    "    print(\"=== Evaluating Feedback Loop Impact ===\")\n",
    "    \n",
    "    # 임시 피드백 파일 생성 (이 평가 세션에만 사용)\n",
    "    temp_feedback_file = \"temp_evaluation_feedback.json\"\n",
    "    \n",
    "    # 피드백 수집 초기화 (시작 시 비어 있음)\n",
    "    feedback_data = []\n",
    "    \n",
    "    # ----------------------- FIRST EVALUATION ROUND -----------------------\n",
    "    # 모든 쿼리를 피드백 영향 없이 실행하여 기준 성능 설정\n",
    "    print(\"\\n=== ROUND 1: NO FEEDBACK ===\")\n",
    "    round1_results = []\n",
    "    \n",
    "    for i, query in enumerate(test_queries):\n",
    "        print(f\"\\nQuery {i+1}: {query}\")\n",
    "        \n",
    "        # 문서 처리를 통해 초기 벡터 저장소 생성\n",
    "        chunks, vector_store = process_document(file_path)\n",
    "        \n",
    "        # 피드백 영향 없이 RAG 실행 (비어 있는 피드백 목록)\n",
    "        result = rag_with_feedback_loop(query, vector_store, [])\n",
    "        round1_results.append(result)\n",
    "        \n",
    "        # 기준 답변이 있다면 인위적인 피드백 생성\n",
    "        # 이는 시스템 학습을 위한 사용자 피드백을 모방합니다\n",
    "        if reference_answers and i < len(reference_answers):\n",
    "            # 기준 답변과의 유사도를 기반으로 인위적인 피드백 점수 계산\n",
    "            similarity_to_ref = calculate_similarity(result[\"response\"], reference_answers[i])\n",
    "            # 유사도 (0-1)를 등급 범위 (1-5)로 변환\n",
    "            relevance = max(1, min(5, int(similarity_to_ref * 5)))\n",
    "            quality = max(1, min(5, int(similarity_to_ref * 5)))\n",
    "            \n",
    "            # 구조화된 피드백 항목 생성\n",
    "            feedback = get_user_feedback(\n",
    "                query=query,\n",
    "                response=result[\"response\"],\n",
    "                relevance=relevance,\n",
    "                quality=quality,\n",
    "                comments=f\"Synthetic feedback based on reference similarity: {similarity_to_ref:.2f}\"\n",
    "            )\n",
    "            \n",
    "            # 메모리에 추가하고 임시 파일에 저장\n",
    "            feedback_data.append(feedback)\n",
    "            store_feedback(feedback, temp_feedback_file)\n",
    "    \n",
    "    # ----------------------- SECOND EVALUATION ROUND -----------------------\n",
    "    # 피드백 통합을 포함하여 동일한 쿼리를 실행하여 개선 측정\n",
    "    print(\"\\n=== ROUND 2: WITH FEEDBACK ===\")\n",
    "    round2_results = []\n",
    "    \n",
    "    # 문서 처리 및 피드백 기반 콘텐츠 활용\n",
    "    chunks, vector_store = process_document(file_path)\n",
    "    vector_store = fine_tune_index(vector_store, chunks, feedback_data)\n",
    "    \n",
    "    for i, query in enumerate(test_queries):\n",
    "        print(f\"\\nQuery {i+1}: {query}\")\n",
    "        \n",
    "        # 피드백 영향을 포함하여 RAG 실행\n",
    "        result = rag_with_feedback_loop(query, vector_store, feedback_data)\n",
    "        round2_results.append(result)\n",
    "    \n",
    "    # ----------------------- RESULTS ANALYSIS -----------------------\n",
    "    # 두 라운드 간 성능 지표 비교\n",
    "    comparison = compare_results(test_queries, round1_results, round2_results, reference_answers)\n",
    "    \n",
    "    # 임시 평가 아티팩트 정리\n",
    "    if os.path.exists(temp_feedback_file):\n",
    "        os.remove(temp_feedback_file)\n",
    "    \n",
    "    return {\n",
    "        \"round1_results\": round1_results,\n",
    "        \"round2_results\": round2_results,\n",
    "        \"comparison\": comparison\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper Functions for Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_similarity(text1, text2):\n",
    "    \"\"\"\n",
    "    두 텍스트 간 의미적 유사도를 임베딩을 사용하여 계산합니다.\n",
    "    \n",
    "    Args:\n",
    "        text1 (str): 첫 번째 텍스트\n",
    "        text2 (str): 두 번째 텍스트\n",
    "        \n",
    "    Returns:\n",
    "        float: 0과 1 사이의 유사도 점수\n",
    "    \"\"\"\n",
    "    # 두 텍스트의 임베딩 생성\n",
    "    embedding1 = create_embeddings(embedding_model, [text1], device=device, batch_size=1)[0]\n",
    "    embedding2 = create_embeddings(embedding_model, [text2], device=device, batch_size=1)[0]\n",
    "    \n",
    "    # 임베딩을 numpy 배열로 변환\n",
    "    vec1 = np.array(embedding1)\n",
    "    vec2 = np.array(embedding2)\n",
    "    \n",
    "    # 두 벡터 간 코사인 유사도 계산\n",
    "    similarity = np.dot(vec1, vec2) / (np.linalg.norm(vec1) * np.linalg.norm(vec2))\n",
    "    \n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_results(queries, round1_results, round2_results, reference_answers=None):\n",
    "    \"\"\"\n",
    "    Compare results from two rounds of RAG.\n",
    "    \n",
    "    Args:\n",
    "        queries (List[str]): Test queries\n",
    "        round1_results (List[Dict]): Results from round 1\n",
    "        round2_results (List[Dict]): Results from round 2\n",
    "        reference_answers (List[str], optional): Reference answers\n",
    "        \n",
    "    Returns:\n",
    "        str: Comparison analysis\n",
    "    \"\"\"\n",
    "    print(\"\\n=== COMPARING RESULTS ===\")\n",
    "    \n",
    "    # 시스템 프롬프트\n",
    "    system_prompt = \"\"\"You are an expert evaluator of RAG systems. Compare responses from two versions:\n",
    "        1. Standard RAG: No feedback used\n",
    "        2. Feedback-enhanced RAG: Uses a feedback loop to improve retrieval\n",
    "\n",
    "        Analyze which version provides better responses in terms of:\n",
    "        - Relevance to the query\n",
    "        - Accuracy of information\n",
    "        - Completeness\n",
    "        - Clarity and conciseness\n",
    "    \"\"\"\n",
    "\n",
    "    comparisons = []\n",
    "    \n",
    "    # 각 쿼리와 두 라운드의 결과에 대해 반복\n",
    "    for i, (query, r1, r2) in enumerate(zip(queries, round1_results, round2_results)):\n",
    "        # 응답 비교를 위한 프롬프트 생성\n",
    "        comparison_prompt = f\"\"\"\n",
    "        Query: {query}\n",
    "\n",
    "        Standard RAG Response:\n",
    "        {r1[\"response\"]}\n",
    "\n",
    "        Feedback-enhanced RAG Response:\n",
    "        {r2[\"response\"]}\n",
    "        \"\"\"\n",
    "\n",
    "        # 기준 답변이 있다면 포함\n",
    "        if reference_answers and i < len(reference_answers):\n",
    "            comparison_prompt += f\"\"\"\n",
    "            Reference Answer:\n",
    "            {reference_answers[i]}\n",
    "            \"\"\"\n",
    "\n",
    "        comparison_prompt += \"\"\"\n",
    "        Compare these responses and explain which one is better and why.\n",
    "        Focus specifically on how the feedback loop has (or hasn't) improved the response quality.\n",
    "        \"\"\"\n",
    "\n",
    "        # OpenAI API를 호출하여 비교 분석 생성\n",
    "        response = client_openai.chat.completions.create(\n",
    "            model=\"gpt-4.1-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": comparison_prompt}\n",
    "            ],\n",
    "            temperature=0\n",
    "        )\n",
    "        \n",
    "        # 비교 분석을 결과에 추가\n",
    "        comparisons.append({\n",
    "            \"query\": query,\n",
    "            \"analysis\": response.choices[0].message.content\n",
    "        })\n",
    "        \n",
    "        # 각 쿼리에 대한 분석 조각 출력\n",
    "        print(f\"\\nQuery {i+1}: {query}\")\n",
    "        print(f\"Analysis: {response.choices[0].message.content[:200]}...\")\n",
    "    \n",
    "    return comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation of the feedback loop (Custom Validation Queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# 평가 데이터 로드하기\n",
    "df = pd.read_csv('./data_creation/rag_val_new_post.csv')\n",
    "\n",
    "# test queries 정의\n",
    "test_queries = [\n",
    "    df['query'][0],\n",
    "    # df['query'][1],\n",
    "    # df['query'][2],\n",
    "    # df['query'][3],\n",
    "    # df['query'][4],\n",
    "    # df['query'][5],\n",
    "    # df['query'][6],\n",
    "    # df['query'][7]\n",
    "    # ...\n",
    "]\n",
    "\n",
    "# 평가를 위한 기준 답변 정의\n",
    "reference_answers = [\n",
    "    df['generation_gt'][0],\n",
    "    # df['generation_gt'][1],\n",
    "    # df['generation_gt'][2],\n",
    "    # df['generation_gt'][3],\n",
    "    # df['generation_gt'][4],\n",
    "    # df['generation_gt'][5],\n",
    "    # df['generation_gt'][6],\n",
    "    # df['generation_gt'][7],\n",
    "    # ...\n",
    "    ]\n",
    "\n",
    "\n",
    "# 파일 경로\n",
    "file_path = \"./data_creation/pdf_data/(1) 2024 달라지는 세금제도.txt\"\n",
    "\n",
    "# 평가 실행\n",
    "evaluation_results = evaluate_feedback_loop(\n",
    "    file_path=file_path,\n",
    "    test_queries=test_queries,\n",
    "    reference_answers=reference_answers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################\n",
    "# # Run a full RAG workflow\n",
    "########################################\n",
    "\n",
    "# # Run an interactive example\n",
    "# print(\"\\n\\n=== INTERACTIVE EXAMPLE ===\")\n",
    "# print(\"Enter your query about AI:\")\n",
    "# user_query = input()\n",
    "\n",
    "# # Load accumulated feedback\n",
    "# all_feedback = load_feedback_data()\n",
    "\n",
    "# # Run full workflow\n",
    "# result = full_rag_workflow(\n",
    "#     pdf_path=pdf_path,\n",
    "#     query=user_query,\n",
    "#     feedback_data=all_feedback,\n",
    "#     fine_tune=True\n",
    "# )\n",
    "\n",
    "########################################\n",
    "# # Run a full RAG workflow\n",
    "########################################"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
