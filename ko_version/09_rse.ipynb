{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "source": [
    "# Relevant Segment Extraction (RSE) for Enhanced RAG\n",
    "\n",
    "이 노트북에서는 RAG 시스템의 컨텍스트 품질을 향상시키기 위해 Relevant Segment Extraction(RSE) 기법을 구현합니다. 단순히 분리된 청크들을 검색하는 대신, 언어 모델에 더 나은 컨텍스트를 제공하는 연속적인 텍스트 세그먼트를 식별하고 재구성합니다.\n",
    "\n",
    "## Key Concept\n",
    "\n",
    "관련 청크들은 문서 내에서 함께 클러스터링되는 경향이 있습니다. 이러한 클러스터를 식별하고 연속성을 유지함으로써 LLM이 작업할 수 있는 더 일관된 컨텍스트를 제공합니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 환경 설정하기\n",
    "필요한 라이브러리를 가져옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import re\n",
    "\n",
    "# .env 파일 로드\n",
    "load_dotenv()\n",
    "\n",
    "API_KEY = os.environ.get('OPENAI_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PDF 파일에서 텍스트 추출하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import google.generativeai as genai\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    # API 키 설정\n",
    "    genai.configure(api_key=gemini_API_KEY)\n",
    "    client = genai.GenerativeModel('gemini-2.0-flash-lite')\n",
    "\n",
    "    # PDF 파일 업로드\n",
    "    with open(pdf_path, \"rb\") as file:\n",
    "        file_data = file.read()\n",
    "\n",
    "\n",
    "    prompt = \"Extract all text from the provided PDF file.\"\n",
    "    response = client.generate_content([\n",
    "        {\"mime_type\": \"application/pdf\", \"data\": file_data},\n",
    "        prompt\n",
    "    ],generation_config={\n",
    "            \"max_output_tokens\": 8192  # 최대 출력 토큰 수 설정 (예: 8192 토큰, 약 24,000~32,000자)\n",
    "    })\n",
    "    return response.text\n",
    "    # Open the PDF file\n",
    "    mypdf = fitz.open(pdf_path)\n",
    "    all_text = \"\"  # Initialize an empty string to store the extracted text\n",
    "\n",
    "    # Iterate through each page in the PDF\n",
    "    for page_num in range(mypdf.page_count):\n",
    "        page = mypdf[page_num]  # Get the page\n",
    "        text = page.get_text(\"text\")  # Extract text from the page\n",
    "        all_text += text  # Append the extracted text to the all_text string\n",
    "\n",
    "    return all_text  # Return the extracted text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023 핵심 개정세법\n",
      "01\n",
      "2024 달라지는 세금제도\n",
      "(국민·기업 납세자용)\n",
      "\n",
      "[유의사항]\n",
      "'2023 핵심 개정세법'은 국회에서 의결된 세법 개정사항을 모두 포괄하고 있으나, 시행령·\n",
      "시행규칙의 경우 2023.7월 발표한 ‘2023 세법개정안'을 중심으로 개정세법과 관련된 내용의\n",
      "경우 그대로 반영하였습니다. 그러므로 정부의 시행령, 시행규칙 개정 과정에서 일부 변동될\n",
      "수 있고 새로이 추가 제정될 수도 있습니다. 아울러 실무상 적용할 때는 반드시 개정세법의\n",
      "구체적인 조문을 확인하셔야 합니다.\n",
      "\n",
      "국민·기업 납세자용\n",
      "2024 달라지는 세금제도\n",
      "2024 달라지는 세금제도\n",
      "2023 세목별 핵심 개정세법\n",
      "2023 개정세법 현행-개정사항 비교\n",
      "01\n",
      "부동산 세금제도\n",
      "●(조특법) 연 단위 양도세 감면한도 악용방지를 위해 감면한도 산정방법 조정\n",
      "양도소득세 산정 및 감면이 연단위로 이뤄지는 점을 감안하여 ▲토지의 일부를\n",
      "양도한 날부터 소급하여 1년 내 토지를 분할한 경우 분필한 토지 또는 토지 지분\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 이미 text 파일로 저장되어 있다면 load_text_file 함수를 사용하면 됩니다.\n",
    "def load_text_file(pdf_path):\n",
    "\n",
    "    # text 파일 로드\n",
    "    with open(pdf_path, \"r\", encoding=\"utf-8\") as txt_file:\n",
    "        text = txt_file.read()\n",
    "\n",
    "    return text\n",
    "\n",
    "txt_path = \"./data_creation/pdf_data/(1) 2024 달라지는 세금제도.txt\"\n",
    "\n",
    "extracted_text = load_text_file(txt_path)\n",
    "print(extracted_text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 추출된 텍스트 chunking\n",
    "pdf 에서 추출된 텍스트를 얻은 후, 검색 정확성을 향상시키기 위해 이를 더 작고 겹치는 청크로 나눕니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_text(text, n, overlap):\n",
    "    \"\"\"\n",
    "    주어진 텍스트를 겹치는 n개의 문자 세그먼트로 청크합니다.\n",
    "\n",
    "    Args:\n",
    "    text (str): 청크할 텍스트입니다.\n",
    "    n (int): 각 청크의 문자 수입니다.\n",
    "    overlap (int): 청크 간 겹치는 문자 수입니다.\n",
    "\n",
    "    Returns:\n",
    "    List[str]: 청크된 텍스트 리스트입니다.\n",
    "    \"\"\"\n",
    "    chunks = []  # 청크된 텍스트를 저장할 빈 리스트를 초기화합니다.\n",
    "    \n",
    "    # (n - overlap) 단계로 텍스트를 반복합니다.\n",
    "    for i in range(0, len(text), n - overlap):\n",
    "        # 인덱스 i부터 i + n까지의 텍스트를 청크 리스트에 추가합니다.\n",
    "        chunks.append(text[i:i + n])\n",
    "\n",
    "    return chunks  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI API 클라이언트 설정하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client_openai = OpenAI(api_key = API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Vector Store 구축\n",
    "NumPy를 사용하여 간단한 Vecotr store 구축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class SimpleVectorStore:\n",
    "    \"\"\"\n",
    "    NumPy를 사용하여 간단한 Vecotr store 구축\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        벡터 저장소 초기화\n",
    "        \"\"\"\n",
    "        self.vectors = []\n",
    "        self.texts = []\n",
    "        self.metadata = []\n",
    "    \n",
    "    def add_item(self, text, embedding, metadata=None):\n",
    "        \"\"\"\n",
    "        벡터 저장소에 항목 추가\n",
    "\n",
    "        Args:\n",
    "        text (str): 원본 텍스트.\n",
    "        embedding (List[float]): 임베딩 벡터.\n",
    "        metadata (dict, optional): 추가 메타데이터.\n",
    "        \"\"\"\n",
    "        self.vectors.append(np.array(embedding))\n",
    "        self.texts.append(text)\n",
    "        self.metadata.append(metadata or {})\n",
    "    \n",
    "    def similarity_search(self, query_embedding, top_k=5):\n",
    "        \"\"\"\n",
    "        시맨틱 서치 수행\n",
    "\n",
    "        Args:\n",
    "        query_embedding (List[float]): 쿼리 임베딩 벡터.\n",
    "        k (int): 반환할 결과의 수.\n",
    "\n",
    "        Returns:\n",
    "        List[Dict]: 텍스트와 메타데이터가 포함된 상위 k개 유사 항목.\n",
    "        \"\"\"\n",
    "        if not self.vectors:\n",
    "            return []\n",
    "        \n",
    "        # 쿼리 임베딩을 numpy 배열로 변환\n",
    "        query_vector = np.array(query_embedding)\n",
    "        \n",
    "        # 코사인 유사도를 사용하여 유사도 계산\n",
    "        similarities = []\n",
    "        for i, vector in enumerate(self.vectors):\n",
    "            similarity = np.dot(query_vector, vector) / (np.linalg.norm(query_vector) * np.linalg.norm(vector))\n",
    "            similarities.append((i, similarity))\n",
    "        \n",
    "        # 유사도에 따라 정렬 (내림차순)\n",
    "        similarities.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        # 상위 k개 결과 반환\n",
    "        results = []\n",
    "        for i in range(min(top_k, len(similarities))):\n",
    "            idx, score = similarities[i]\n",
    "            results.append({\n",
    "                \"text\": self.texts[idx],\n",
    "                \"metadata\": self.metadata[idx],\n",
    "                \"similarity\": score\n",
    "            })\n",
    "        \n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 임베딩 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "def create_embeddings(embedding_model, texts, device='cuda', batch_size=16):\n",
    "    \"\"\"\n",
    "    SentenceTransformer 모델을 사용하여 지정된 텍스트에 대한 임베딩을 생성합니다.\n",
    "\n",
    "    Args:\n",
    "        embedding_model: 임베딩을 생성할 SentenceTransformer 모델입니다.\n",
    "        texts (list): 임베딩을 생성할 입력 텍스트 리스트입니다.\n",
    "        device (str): 모델을 실행할 장치 ('cuda' for GPU, 'cpu' for CPU).\n",
    "        batch_size (int): 인코딩을 위한 배치 크기입니다.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: 모델에 의해 생성된 임베딩입니다.\n",
    "    \"\"\"\n",
    "    # 모델이 지정된 장치에 있는지 확인합니다.\n",
    "    embedding_model = embedding_model.to(device)\n",
    "    \n",
    "    # 지정된 배치 크기로 임베딩을 생성합니다.\n",
    "    embeddings = embedding_model.encode(\n",
    "        texts,\n",
    "        device=device,\n",
    "        batch_size=batch_size,  # 메모리 사용량을 줄이기 위해 더 작은 배치 크기를 사용합니다.\n",
    "        show_progress_bar=True  # 인코딩 진행 상태를 모니터링하기 위한 진행 표시줄을 표시합니다.\n",
    "    )\n",
    "    \n",
    "    return embeddings\n",
    "\n",
    "# GPU 사용 가능 여부를 확인합니다.\n",
    "device = 'cuda' if torch.cuda.is_available() else 'mps'\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# 모델을 로드합니다.\n",
    "model = \"BAAI/bge-m3\"\n",
    "embedding_model = SentenceTransformer(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RSE를 사용한 문서 처리\n",
    "이제 핵심 RSE 기능을 구현해 보겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_document(file_path, chunk_size=800):\n",
    "    \"\"\"\n",
    "    문서를 전처리하여 RSE 시스템에 적합한 형태로 변환합니다.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): 파일 경로\n",
    "        chunk_size (int): 각 청크의 크기\n",
    "        \n",
    "    Returns:\n",
    "        Tuple[List[str], SimpleVectorStore, Dict]: chunk, 벡터 저장소, 문서 정보\n",
    "    \"\"\"\n",
    "    print(\"Extracting text from document...\")\n",
    "    # PDF 파일에서 텍스트 추출\n",
    "    # text = extract_text_from_pdf(pdf_path)\n",
    "\n",
    "    # 텍스트 파일 로드\n",
    "    extracted_text = load_text_file(file_path)\n",
    "    \n",
    "    print(\"Chunking text into non-overlapping segments...\")\n",
    "    # 추출된 텍스트를 청크로 나눕니다.\n",
    "    chunks = chunk_text(extracted_text, chunk_size, overlap=0)\n",
    "    print(f\"Created {len(chunks)} chunks\")\n",
    "    \n",
    "    print(\"Generating embeddings for chunks...\")\n",
    "    # 텍스트 청크의 임베딩 생성\n",
    "    chunk_embeddings = create_embeddings(embedding_model, chunks, device=device, batch_size=1)\n",
    "    \n",
    "    # 벡터 저장소 생성\n",
    "    vector_store = SimpleVectorStore()\n",
    "    \n",
    "    # 각 청크와 임베딩을 vector store에 저장\n",
    "    metadata = [{\"chunk_index\": i, \"source\": file_path} for i in range(len(chunks))]\n",
    "    vector_store.add_item(chunks, chunk_embeddings, metadata)\n",
    "    \n",
    "    # 원본 문서 구조 추적\n",
    "    doc_info = {\n",
    "        \"chunks\": chunks,\n",
    "        \"source\": file_path,\n",
    "    }\n",
    "    \n",
    "    return chunks, vector_store, doc_info"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RSE 핵심 알고리즘: 청크 값 계산 및 최적 세그먼트 찾기\n",
    "이제 문서를 처리하고 청크에 대한 임베딩을 생성하는 데 필요한 함수를 준비했으므로, RSE의 핵심 알고리즘을 구현할 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_chunk_values(query, chunks, vector_store, irrelevant_chunk_penalty=0.2):\n",
    "    \"\"\"\n",
    "    관련성과 위치를 결합하여 청크 값을 계산합니다.\n",
    "    \n",
    "    Args:\n",
    "        query (str): 쿼리 텍스트\n",
    "        chunks (List[str]): 문서 청크 리스트\n",
    "        vector_store (SimpleVectorStore): 청크가 포함된 벡터 저장소\n",
    "        irrelevant_chunk_penalty (float): 관련성이 없는 청크에 대한 패널티\n",
    "        \n",
    "    Returns:\n",
    "        List[float]: 청크 값 리스트\n",
    "    \"\"\"\n",
    "    # 쿼리 임베딩 생성\n",
    "    query_embedding = create_embeddings(embedding_model, [query], device=device, batch_size=1)[0]\n",
    "    \n",
    "    # 유사도 점수가 있는 모든 청크 가져오기\n",
    "    num_chunks = len(chunks)\n",
    "    results = vector_store.similarity_search(query_embedding, top_k=num_chunks)\n",
    "    \n",
    "    # 청크 인덱스와 관련성 점수 매핑\n",
    "    relevance_scores = {result[\"metadata\"][\"chunk_index\"]: result[\"score\"] for result in results}\n",
    "    \n",
    "    # 청크 값 계산 (관련성 점수에서 패널티 적용)\n",
    "    chunk_values = []\n",
    "    for i in range(num_chunks):\n",
    "        # 관련성 점수 가져오기 또는 결과에 없는 경우 0으로 설정\n",
    "        score = relevance_scores.get(i, 0.0)\n",
    "        # 관련성이 없는 청크에 대한 패널티 적용\n",
    "        value = score - irrelevant_chunk_penalty\n",
    "        chunk_values.append(value)\n",
    "    \n",
    "    return chunk_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_segments(chunk_values, max_segment_length=20, total_max_length=30, min_segment_value=0.2):\n",
    "    \"\"\"\n",
    "    최대 합 하위 배열 알고리즘의 변형을 사용하여 최적의 연속 텍스트 세그먼트를 찾습니다.\n",
    "    \n",
    "    Args:\n",
    "        chunk_values (List[float]): 각 청크의 값\n",
    "        max_segment_length (int): 단일 세그먼트의 최대 길이\n",
    "        total_max_length (int): 모든 세그먼트에 대한 최대 총 길이\n",
    "        min_segment_value (float): 세그먼트로 간주되기 위한 최소 값\n",
    "        \n",
    "    Returns:\n",
    "        List[Tuple[int, int]]: 최적 세그먼트의 (start, end) 인덱스 리스트\n",
    "    \"\"\"\n",
    "    print(\"Finding optimal continuous text segments...\")\n",
    "    \n",
    "    best_segments = []\n",
    "    segment_scores = []\n",
    "    total_included_chunks = 0\n",
    "    \n",
    "    # 세그먼트 찾기\n",
    "    while total_included_chunks < total_max_length:\n",
    "        best_score = min_segment_value  # 세그먼트로 간주되기 위한 최소 값  \n",
    "        best_segment = None\n",
    "        \n",
    "        # 가능한 모든 시작 위치 시도\n",
    "        for start in range(len(chunk_values)):\n",
    "            # 이미 선택된 세그먼트에 포함된 경우 건너뜁니다.\n",
    "            if any(start >= s[0] and start < s[1] for s in best_segments):\n",
    "                continue\n",
    "                \n",
    "            # 가능한 모든 세그먼트 길이 시도\n",
    "            for length in range(1, min(max_segment_length, len(chunk_values) - start) + 1):\n",
    "                end = start + length\n",
    "                \n",
    "                # 이미 선택된 세그먼트에 포함된 경우 건너뜁니다.\n",
    "                if any(end > s[0] and end <= s[1] for s in best_segments):\n",
    "                    continue\n",
    "                \n",
    "                # 세그먼트 값을 청크 값의 합으로 계산\n",
    "                segment_value = sum(chunk_values[start:end])\n",
    "                \n",
    "                # 이 세그먼트가 더 좋은 경우 업데이트\n",
    "                if segment_value > best_score:\n",
    "                    best_score = segment_value\n",
    "                    best_segment = (start, end)\n",
    "        \n",
    "        # 좋은 세그먼트를 찾은 경우 추가\n",
    "        if best_segment:\n",
    "            best_segments.append(best_segment)\n",
    "            segment_scores.append(best_score)\n",
    "            total_included_chunks += best_segment[1] - best_segment[0]\n",
    "            print(f\"Found segment {best_segment} with score {best_score:.4f}\")\n",
    "        else:\n",
    "            # 더 좋은 세그먼트를 찾을 수 없음   \n",
    "            break\n",
    "    \n",
    "    # 읽기 쉽도록 세그먼트를 시작 위치에 따라 정렬\n",
    "    best_segments = sorted(best_segments, key=lambda x: x[0])\n",
    "    \n",
    "    return best_segments, segment_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconstructing and Using Segments for RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_segments(chunks, best_segments):\n",
    "    \"\"\"\n",
    "    청크 인덱스를 기반으로 텍스트 세그먼트를 재구성합니다.\n",
    "    \n",
    "    Args:\n",
    "        chunks (List[str]): 모든 문서 청크 리스트\n",
    "        best_segments (List[Tuple[int, int]]): 세그먼트의 (start, end) 인덱스 리스트\n",
    "        \n",
    "    Returns:\n",
    "        List[str]: 재구성된 텍스트 세그먼트 리스트\n",
    "    \"\"\"\n",
    "    reconstructed_segments = []  # 재구성된 세그먼트를 저장할 빈 리스트 초기화\n",
    "    \n",
    "    for start, end in best_segments:\n",
    "        # 이 세그먼트의 청크를 결합하여 완전한 세그먼트 텍스트 생성\n",
    "        segment_text = \" \".join(chunks[start:end])\n",
    "        # 세그먼트 텍스트와 범위를 reconstructed_segments 리스트에 추가\n",
    "        reconstructed_segments.append({\n",
    "            \"text\": segment_text,\n",
    "            \"segment_range\": (start, end),\n",
    "        })\n",
    "    \n",
    "    return reconstructed_segments  # 재구성된 텍스트 세그먼트 리스트 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_segments_for_context(segments):\n",
    "    \"\"\"\n",
    "    세그먼트를 LLM에 대한 컨텍스트 문자열로 포맷합니다.\n",
    "    \n",
    "    Args:\n",
    "        segments (List[Dict]): 세그먼트 딕셔너리 리스트\n",
    "        \n",
    "    Returns:\n",
    "        str: 포맷팅된 컨텍스트 텍스트\n",
    "    \"\"\"\n",
    "    context = []  # 포맷팅된 컨텍스트를 저장할 빈 리스트 초기화\n",
    "    \n",
    "    for i, segment in enumerate(segments):\n",
    "        # 각 세그먼트에 대한 헤더 생성 (인덱스와 청크 범위)\n",
    "        segment_header = f\"SEGMENT {i+1} (Chunks {segment['segment_range'][0]}-{segment['segment_range'][1]-1}):\"\n",
    "        context.append(segment_header)  # 세그먼트 헤더를 컨텍스트 리스트에 추가\n",
    "        context.append(segment['text'])  # 세그먼트 텍스트를 컨텍스트 리스트에 추가\n",
    "        context.append(\"-\" * 80)  # 구분 선 추가\n",
    "    \n",
    "    # 컨텍스트 리스트의 모든 요소를 더블 뉴라인으로 결합하여 결과 반환\n",
    "    return \"\\n\\n\".join(context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Responses with RSE Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(query, context, model_name='gpt-4.1-mini'):\n",
    "\n",
    "    # AI 어시스턴트의 시스템 프롬프트를 정의합니다.\n",
    "    system_prompt = \"당신은 제공된 Context에 기반하여 답변하는 AI 어시스턴트입니다. 답변이 컨텍스트에서 직접 도출될 수 없는 경우, 다음 문장을 사용하세요: '해당 질문에 답변할 충분한 정보가 없습니다.'\"\n",
    "    \n",
    "    user_prompt = f\"\"\"\n",
    "        Context:\n",
    "        {context}\n",
    "\n",
    "        Question: {query}\n",
    "\n",
    "        Please answer the question based only on the context provided above. Be concise and accurate.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = client_openai.chat.completions.create(\n",
    "        model=model_name,\n",
    "        temperature=0.1,\n",
    "        top_p=0.9,\n",
    "        max_tokens=1024,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Complete RSE Pipeline Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag_with_rse(pdf_path, query, chunk_size=800, irrelevant_chunk_penalty=0.2):\n",
    "    \"\"\"\n",
    "    RSE를 사용한 RAG 파이프라인을 구현합니다.\n",
    "    \n",
    "    Args:\n",
    "        pdf_path (str): 문서 경로\n",
    "        query (str): 사용자 쿼리\n",
    "        chunk_size (int): 청크 크기\n",
    "        irrelevant_chunk_penalty (float): 관련성이 없는 청크에 대한 패널티\n",
    "        \n",
    "    Returns:\n",
    "        Dict: 쿼리, 세그먼트, 응답이 포함된 결과\n",
    "    \"\"\"\n",
    "    print(\"\\n=== STARTING RAG WITH RELEVANT SEGMENT EXTRACTION ===\")\n",
    "    print(f\"Query: {query}\")\n",
    "    \n",
    "    # 문서를 처리하여 텍스트 추출, 청크로 나누고 임베딩 생성\n",
    "    chunks, vector_store, doc_info = process_document(pdf_path, chunk_size)\n",
    "    \n",
    "    # 쿼리에 대한 관련성 점수와 청크 값 계산\n",
    "    print(\"\\nCalculating relevance scores and chunk values...\")\n",
    "    chunk_values = calculate_chunk_values(query, chunks, vector_store, irrelevant_chunk_penalty)\n",
    "    \n",
    "    # 청크 값에 따라 텍스트 세그먼트 찾기\n",
    "    best_segments, scores = find_best_segments(\n",
    "        chunk_values, \n",
    "        max_segment_length=20, \n",
    "        total_max_length=30, \n",
    "        min_segment_value=0.2\n",
    "    )\n",
    "    \n",
    "    # 최적 청크에서 텍스트 세그먼트 재구성\n",
    "    print(\"\\nReconstructing text segments from chunks...\")\n",
    "    segments = reconstruct_segments(chunks, best_segments)\n",
    "    \n",
    "    # 세그먼트를 언어 모델에 대한 컨텍스트 문자열로 포맷\n",
    "    context = format_segments_for_context(segments)\n",
    "    \n",
    "    # 컨텍스트를 기반으로 응답 생성\n",
    "    response = generate_response(query, context)\n",
    "    \n",
    "    # 결과를 딕셔너리로 컴파일\n",
    "    result = {\n",
    "        \"query\": query,\n",
    "        \"segments\": segments,\n",
    "        \"response\": response\n",
    "    }\n",
    "    \n",
    "    print(\"\\n=== FINAL RESPONSE ===\")\n",
    "    print(response)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 표준 검색 방식과의 비교\n",
    "RSE와 비교하기 위해 표준 검색 방식을 구현해 보겠습니다:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standard_top_k_retrieval(file_path, query, k=10, chunk_size=800):\n",
    "    \"\"\"\n",
    "    표준 RAG 파이프라인을 구현합니다.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): 문서 경로\n",
    "        query (str): 사용자 쿼리\n",
    "        k (int): 검색할 청크 수\n",
    "        chunk_size (int): 청크 크기\n",
    "        \n",
    "    Returns:\n",
    "        Dict: 쿼리, 청크, 응답이 포함된 결과\n",
    "    \"\"\"\n",
    "    print(\"\\n=== STARTING STANDARD TOP-K RETRIEVAL ===\")\n",
    "    print(f\"Query: {query}\")\n",
    "    \n",
    "    # 문서를 처리하여 텍스트 추출, 청크로 나누고 임베딩 생성\n",
    "    chunks, vector_store, doc_info = process_document(file_path, chunk_size)\n",
    "    \n",
    "    # 쿼리에 대한 임베딩 생성\n",
    "    print(\"Creating query embedding and retrieving chunks...\")\n",
    "    query_embedding = create_embeddings(embedding_model, [query], device=device, batch_size=1)[0]\n",
    "    \n",
    "    # 쿼리 임베딩에 따라 상위 k개의 관련성 높은 청크 검색\n",
    "    results = vector_store.search(query_embedding, top_k=k)\n",
    "    retrieved_chunks = [result[\"text\"] for result in results]\n",
    "    \n",
    "    # 검색된 청크를 컨텍스트 문자열로 포맷\n",
    "    context = \"\\n\\n\".join([\n",
    "        f\"CHUNK {i+1}:\\n{chunk}\" \n",
    "        for i, chunk in enumerate(retrieved_chunks)\n",
    "    ])\n",
    "    \n",
    "    # 컨텍스트를 기반으로 응답 생성\n",
    "    response = generate_response(query, context)\n",
    "    \n",
    "    # 결과를 딕셔너리로 컴파일\n",
    "    result = {\n",
    "        \"query\": query,\n",
    "        \"chunks\": retrieved_chunks,\n",
    "        \"response\": response\n",
    "    }\n",
    "    \n",
    "    print(\"\\n=== FINAL RESPONSE ===\")\n",
    "    print(response)\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RSE 기법 평가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_methods(file_path, query, reference_answer=None):\n",
    "    \"\"\"\n",
    "    RSE와 표준 RAG 비교\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): 문서 경로\n",
    "        query (str): 사용자 쿼리\n",
    "        reference_answer (str, optional): 참조 답변\n",
    "    \"\"\"\n",
    "    print(\"\\n========= EVALUATION =========\\n\")\n",
    "    \n",
    "    # RSE 방법 실행\n",
    "    rse_result = rag_with_rse(file_path, query)\n",
    "    \n",
    "    # 표준 RAG 방법 실행\n",
    "    standard_result = standard_top_k_retrieval(file_path, query)\n",
    "    \n",
    "    # 참조 답변이 제공된 경우 응답 평가\n",
    "    if reference_answer:\n",
    "        print(\"\\n=== COMPARING RESULTS ===\")\n",
    "        \n",
    "        # 평가 프롬프트 생성\n",
    "        evaluation_prompt = f\"\"\"\n",
    "            Query: {query}\n",
    "\n",
    "            Reference Answer:\n",
    "            {reference_answer}\n",
    "\n",
    "            Response from Standard Retrieval:\n",
    "            {standard_result[\"response\"]}\n",
    "\n",
    "            Response from Relevant Segment Extraction:\n",
    "            {rse_result[\"response\"]}\n",
    "\n",
    "            Compare these two responses against the reference answer. Which one is:\n",
    "            1. More accurate and comprehensive\n",
    "            2. Better at addressing the user's query\n",
    "            3. Less likely to include irrelevant information\n",
    "\n",
    "            Explain your reasoning for each point.\n",
    "        \"\"\"\n",
    "        \n",
    "        print(\"Evaluating responses against reference answer...\")\n",
    "        \n",
    "        # 지정된 모델을 사용하여 평가 생성\n",
    "        evaluation = client_openai.chat.completions.create(\n",
    "            model=\"gpt-4.1-mini\",\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": \"You are an objective evaluator of RAG system responses.\"},\n",
    "                {\"role\": \"user\", \"content\": evaluation_prompt}\n",
    "            ]\n",
    "        )\n",
    "        \n",
    "        # 평가 결과 출력\n",
    "        print(\"\\n=== EVALUATION RESULTS ===\")\n",
    "        print(evaluation.choices[0].message.content)\n",
    "    \n",
    "    # 두 방법의 결과 반환\n",
    "    return {\n",
    "        \"rse_result\": rse_result,\n",
    "        \"standard_result\": standard_result\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "========= EVALUATION =========\n",
      "\n",
      "\n",
      "=== STARTING RAG WITH RELEVANT SEGMENT EXTRACTION ===\n",
      "Query: 피고의 고지의무 위반으로 인해 원고들이 입은 손해는 무엇으로 정의됩니까?\n",
      "Extracting text from document...\n",
      "Chunking text into non-overlapping segments...\n",
      "Created 35 chunks\n",
      "Generating embeddings for chunks...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66081e5892974b19a88e81031028f3e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/35 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Calculating relevance scores and chunk values...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a72e2eea4d0456c846078cfce79dc40",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "shapes (1024,) and (35,1024) not aligned: 1024 (dim 0) != 35 (dim 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[38]\u001b[39m\u001b[32m, line 15\u001b[39m\n\u001b[32m     12\u001b[39m file_path = \u001b[33m\"\u001b[39m\u001b[33m./data_creation/pdf_data/(1) 2024 달라지는 세금제도.txt\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Run evaluation\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m15\u001b[39m results = \u001b[43mevaluate_methods\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrue_answer\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 13\u001b[39m, in \u001b[36mevaluate_methods\u001b[39m\u001b[34m(file_path, query, reference_answer)\u001b[39m\n\u001b[32m     10\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m========= EVALUATION =========\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# RSE 방법 실행\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m rse_result = \u001b[43mrag_with_rse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# 표준 RAG 방법 실행\u001b[39;00m\n\u001b[32m     16\u001b[39m standard_result = standard_top_k_retrieval(file_path, query)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[35]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36mrag_with_rse\u001b[39m\u001b[34m(pdf_path, query, chunk_size, irrelevant_chunk_penalty)\u001b[39m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# 쿼리에 대한 관련성 점수와 청크 값 계산\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mCalculating relevance scores and chunk values...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m chunk_values = \u001b[43mcalculate_chunk_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunks\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvector_store\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mirrelevant_chunk_penalty\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     24\u001b[39m \u001b[38;5;66;03m# 청크 값에 따라 텍스트 세그먼트 찾기\u001b[39;00m\n\u001b[32m     25\u001b[39m best_segments, scores = find_best_segments(\n\u001b[32m     26\u001b[39m     chunk_values, \n\u001b[32m     27\u001b[39m     max_segment_length=\u001b[32m20\u001b[39m, \n\u001b[32m     28\u001b[39m     total_max_length=\u001b[32m30\u001b[39m, \n\u001b[32m     29\u001b[39m     min_segment_value=\u001b[32m0.2\u001b[39m\n\u001b[32m     30\u001b[39m )\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[30]\u001b[39m\u001b[32m, line 19\u001b[39m, in \u001b[36mcalculate_chunk_values\u001b[39m\u001b[34m(query, chunks, vector_store, irrelevant_chunk_penalty)\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# 유사도 점수가 있는 모든 청크 가져오기\u001b[39;00m\n\u001b[32m     18\u001b[39m num_chunks = \u001b[38;5;28mlen\u001b[39m(chunks)\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m results = \u001b[43mvector_store\u001b[49m\u001b[43m.\u001b[49m\u001b[43msimilarity_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_embedding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_chunks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# 청크 인덱스와 관련성 점수 매핑\u001b[39;00m\n\u001b[32m     22\u001b[39m relevance_scores = {result[\u001b[33m\"\u001b[39m\u001b[33mmetadata\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mchunk_index\u001b[39m\u001b[33m\"\u001b[39m]: result[\u001b[33m\"\u001b[39m\u001b[33mscore\u001b[39m\u001b[33m\"\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results}\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 48\u001b[39m, in \u001b[36mSimpleVectorStore.similarity_search\u001b[39m\u001b[34m(self, query_embedding, top_k)\u001b[39m\n\u001b[32m     46\u001b[39m similarities = []\n\u001b[32m     47\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, vector \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m.vectors):\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m     similarity = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mquery_vector\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvector\u001b[49m\u001b[43m)\u001b[49m / (np.linalg.norm(query_vector) * np.linalg.norm(vector))\n\u001b[32m     49\u001b[39m     similarities.append((i, similarity))\n\u001b[32m     51\u001b[39m \u001b[38;5;66;03m# 유사도에 따라 정렬 (내림차순)\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: shapes (1024,) and (35,1024) not aligned: 1024 (dim 0) != 35 (dim 0)"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "# 평가 데이터 로드하기\n",
    "df = pd.read_csv('./data_creation/rag_val_new_post.csv')\n",
    "\n",
    "# 평가 데이터에서 첫 번째 쿼리를 추출합니다.\n",
    "query = df['query'][0]\n",
    "\n",
    "# 평가 데이터에서  정답 답변을 추출합니다.\n",
    "true_answer = df['generation_gt'][0]\n",
    "\n",
    "# 파일 경로\n",
    "file_path = \"./data_creation/pdf_data/(1) 2024 달라지는 세금제도.txt\"\n",
    "\n",
    "# Run evaluation\n",
    "results = evaluate_methods(file_path, query, true_answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
